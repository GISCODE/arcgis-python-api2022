{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a new model using Model Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With arcgis.learn, there are a multitude of machine learning models available for many different tasks. There are models for object detection, pixel classification, image translation, natural language processing, point-cloud data, etc., and the list continues to grow.\n",
    "\n",
    "However, what if you come across a deep learning model that is not yet a part of the learn module, and you want to use it from its library or its open-source code on GitHub? What if you created your own deep learning model for a specific task you are working on? Finally, what if you want to use these new models with all the capabilities of the ArcGIS ecosystem?\n",
    "\n",
    "Fortunately, there is a solution - **Model Extension**, a general-purpose wrapper for any **object detection** and **pixel classification** model on top of our existing framework. It wraps all the details of our stack of PyTorch, Fastai, and the learn module and provides an easy to implement structure for the integration of a third-party deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of a deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first look at the important parts of a deep learning model and then we will correlate them with model extension:\n",
    "\n",
    "1. **Preprocessing of input data** - Raw image pixels are usually not the best way to feed the model. As such, we need to transform the input data and the ground truth information in a way that the model can process. \n",
    "\n",
    "2. **Model Architecture** - This part defines the actual model architecture with different types of neural network layers (convolutional layers, fully connected layers, batch-norm, dropout, etc.) and the connections between them.\n",
    "\n",
    "3. **Loss Calculation** -  A loss function is needed to calculate how much the model predictions are deviating from the ground truth. This calculation helps in adjusting the weights of the layers that train the model to make better predictions.\n",
    "\n",
    "4. **Postprocessing of the outputs** - This step transforms the output from the model so that it can be visualized or understood by a user.\n",
    "\n",
    "The parts described provide a higher level perspective that only describes the aspects pertaining to our requirements for model integration. A deep learning model can be much more complex upon delving deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to use Model Extension, a class needs to be created with a few specific functions. These functions will correspond to the different parts of a deep learning model that we discussed earlier. The class can be given any name, but the required functions need to have the exact names. All of the libraries required need to be imported within this class and used in the functions with a `self` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelName():\n",
    "    import torch\n",
    "    import fastai\n",
    "    import ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three functions that need to be created to preprocess the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `on_batch_begin`: This function is required to transform the input data and the target (the ground truth) used for training the model. The transformation of inputs is in accordance to the model input requirements. This function is equivalent to the fastai on_batch_begin function, but it is called after it. Therefore, transformation of inputs is needed only if the format required by the model is different from what fastai transforms it into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function receives the following arguments:\n",
    "+ `learn` - a fastai learner object\n",
    "\n",
    "+ `model_input_batch` -  fastai transformed batch of input images: tensor of shape [N, C, H, W] with values in the range -1 and 1, where\n",
    "                       N - batch size\n",
    "                       C - number of channels (bands) in the image\n",
    "                       H - height of the image\n",
    "                       W - width of the image  \n",
    "\n",
    "+ `model_target_batch` - fastai transformed batch of targets. The targets will be of different type and shape for object detection and pixel classification.\n",
    "  \n",
    "    **Object Detection**\n",
    "                       \n",
    "                       list of tensors [bboxes, classes]\n",
    "                       \n",
    "                       bboxes: tensor of shape [N, B, 4], where \n",
    "                           N - batch size\n",
    "                           B - the maximum number of boxes present in any image of the batch\n",
    "                           4 - the bounding box coordinates in the order y1, x1, y2, x2 and values in the range -1 to 1\n",
    "                           \n",
    "                       classes: tensor of shape [N, B] representing class of each bounding box\n",
    "\n",
    "    **Pixel Classification**\n",
    "\n",
    "                       tensor of shape [N, K, H, W] representing a binary raster, where\n",
    "\n",
    "                           N - batch size\n",
    "                           K - number of classes in the dataset\n",
    "                           H - height of the image\n",
    "                           W - width of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_batch_begin(self, learn, model_input_batch, model_target_batch):\n",
    "    \"\"\"\n",
    "    Function to transform the input data and the targets in accordance to the model for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_input = ...\n",
    "    model_target = ...\n",
    "\n",
    "    return model_input, model_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `transform_input`: This function is required to transform the input images during inferencing.\n",
    "    \n",
    "    The function receives the following arguments:\n",
    "    + `xb` -  fastai transformed batch of input images: tensor of shape [N, C, H, W], where\n",
    "           N - batch size\n",
    "           C - number of channels (bands) in the image\n",
    "           H - height of the image\n",
    "           W - width of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input(self, xb):\n",
    "    \"\"\"\n",
    "    Function to transform the inputs for inferencing.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_input = ...\n",
    "\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `transform_input_multispectral`: This function is similar to `transform_input`. It is required to transform the input images during inferencing if the data is multispectral imagery. The inputs can be returned as-it-is if multispectral data is not being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_multispectral(self, xb):\n",
    "    \"\"\"\n",
    "    Function to transform the multispectral inputs for inferencing.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_input = ...\n",
    "\n",
    "    return model_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next comes the model architecture. The function `get_model` is used to define the model architecture. Here, you can either create the sequence of neural network layers or import the model defintion from libraries like torchvision, fastai, or a third party repository built using Pytorch. \n",
    "\n",
    "The required arguments are:\n",
    "+ `data`      - This is the databunch created using the prepare_data function.\n",
    "+ `kwargs`    - Any additional parameters required by the model can be used as keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(self, data, **kwargs):\n",
    "    \"\"\"\n",
    "    Function used to define the model architecture.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = ...\n",
    "    self.model = model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During model training, a loss is calculated to ascertain how much the predictions deviate from the ground truth. The loss is then used by the model to backpropagate the required changes in its weights to bring the predictions closer to the target. This function is used to define the loss for the model.\n",
    "\n",
    "The function receives the following arguments:\n",
    "+ `model_output`  -  the predictions by the model\n",
    "+ `*model_target` -  the corresponding ground truth (from the one_batch_begin function explained earlier)\n",
    "        \n",
    "Note: In certain models, the loss calculation is a part of the model definition. In such cases, this function can return the model output as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, model_output, *model_target):\n",
    "    \"\"\"\n",
    "    Function to define the loss calculations.\n",
    "    \"\"\"\n",
    "    final_loss = ...\n",
    "    \n",
    "    return final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing of the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the raw output of the model is not interpretable by the user and needs to be post-processed. The `post_process` function is used to transform the raw outputs of the model to a specific format for the final results and visualization pipeline to ingest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Object Detection**\n",
    "\n",
    "The function receives the following arguments:\n",
    "+ `pred` - Raw output of the model for a batch of images\n",
    "+ `nms_overlap` - Non-maxima suppression value used to select from overlapping bounding boxes\n",
    "+ `thres` - Confidence threshold to be used to filter the predictions\n",
    "+ `chip_size` - Size of the image chips on which predictions are made\n",
    "+ `device` - Device (CPU or GPU) on which the output needs to be put after post-processing\n",
    "\n",
    "Returns:\n",
    "+ `post_processed_pred`: List[Tuple(bboxes, labels, scores)] where for each image\n",
    "                       bboxes - tensor of shape [Number_of_bboxes_in_image, 4]\n",
    "                       labels - tensor of shape [Number_of_bboxes_in_image,]\n",
    "                       scores - tensor of shape [Number_of_bboxes_in_image,]\n",
    "                       \n",
    "                       The bounding box (bboxes) values need to be in range -1 to 1 and in [y1,x1,y2,x2] format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(self, pred, nms_overlap, thres, chip_size, device):\n",
    "        \"\"\"\n",
    "        Fuction to post process the output of the model in validation/infrencing mode.\n",
    "        \"\"\"\n",
    "        post_processed_pred = []\n",
    "        for p in pred:\n",
    "            # Convert bboxes to range -1 to 1.\n",
    "            # Convert bboxes to format [y1,x1,y2,x2]\n",
    "            # Create a tuple of bboxes, labels and scores for each image and append it in a list\n",
    "            ...\n",
    "            post_processed_pred.append(...)\n",
    "            \n",
    "        return post_processed_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pixel Classification**\n",
    "\n",
    "The function receives the following arguments:\n",
    "+ `pred` - Raw output of the model for a batch of images\n",
    "+ `thres` - Confidence threshold to be used to filter the predictions\n",
    "\n",
    "Returns:\n",
    "+ `post_processed_pred`: tensor of shape [N, 1, H, W] or a List/Tuple of N tensors of shape [1, H, W], where\n",
    "                       N - batch size\n",
    "                       H - height of the image\n",
    "                       W - width of the image\n",
    "                       \n",
    "                       The values (type: LongTensor) of the tensor denote the predicted class of each pixel.\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(self, pred, thres):\n",
    "        \"\"\"\n",
    "        Fuction to post process the output of the model in validation/infrencing mode.\n",
    "        \"\"\"\n",
    "        post_processed_pred = ...\n",
    "            \n",
    "        return post_processed_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all of this together in an example. For demonstration purpose we have chosen to integrate the **FasterRCNN** model available in the torchvision library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyFasterRCNN():\n",
    "    \"\"\"\n",
    "    Custom class to integrate FasterRCNN using Model Extension\n",
    "    \"\"\"\n",
    "    \n",
    "    import torch\n",
    "    import torchvision\n",
    "    import fastai\n",
    "\n",
    "\n",
    "    def on_batch_begin(self, learn, model_input_batch, model_target_batch):\n",
    "        \"\"\"\n",
    "        Function to transform the input data and the targets in accordance to the model for training.\n",
    "        \"\"\"\n",
    "        \n",
    "        # During training, after each epoch, validation loss is required on validation dataset\n",
    "        # Torchvision's FasterRCNN model gives losses only in training mode \n",
    "        # and therefore, the model is set to train mode\n",
    "        learn.model.train()\n",
    "\n",
    "        target_list = []\n",
    "\n",
    "        # Denormalize from imagenet_stats\n",
    "        if not learn.data._is_multispectral:\n",
    "            imagenet_stats = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n",
    "            mean = self.torch.tensor(imagenet_stats[0], dtype=self.torch.float32).to(model_input_batch.device)\n",
    "            std  = self.torch.tensor(imagenet_stats[1], dtype=self.torch.float32).to(model_input_batch.device)\n",
    "            model_input_batch = (model_input_batch.permute(0, 2, 3, 1)*std + mean).permute(0, 3, 1, 2)\n",
    "        \n",
    "        for bbox, label in zip(*model_target_batch):\n",
    "            \n",
    "            # FasterRCNN model require bboxes with values between 0 and H and 0 and W.\n",
    "            bbox = ((bbox+1)/2)*learn.data.chip_size \n",
    "            \n",
    "            # FasterRCNN require target of each image in the format of a dictionary.\n",
    "            target = {} \n",
    "            \n",
    "            # Handle images without any bboxes.\n",
    "            if bbox.nelement() == 0:        \n",
    "                bbox = self.torch.tensor([[0.,0.,0.,0.]]).to(learn.data.device)\n",
    "                label = self.torch.tensor([0]).to(learn.data.device)\n",
    "            \n",
    "            # FasterRCNN require the format of bboxes as [x1,y1,x2,y2].\n",
    "            bbox = self.torch.index_select(bbox, 1, self.torch.tensor([1,0,3,2]).to(learn.data.device))\n",
    "            \n",
    "            # FasterRCNN require batch of targets in the form of a list of dictionaries.\n",
    "            target[\"boxes\"] = bbox\n",
    "            target[\"labels\"] = label\n",
    "            target_list.append(target)\n",
    "        \n",
    "        # FasterRCNN require model input with images and coresponding targets in training mode to return the losses\n",
    "        # therefore, append the targets in model_input itself.\n",
    "        model_input = [list(model_input_batch), target_list]\n",
    "        \n",
    "        # Model target is not required in traing mode so just return the same model_target to train the model.\n",
    "        model_target = model_target_batch\n",
    "\n",
    "        return model_input, model_target\n",
    "  \n",
    "\n",
    "    def transform_input(self, xb, thresh=0.5, nms_overlap=0.1):\n",
    "        \"\"\"\n",
    "        Function to transform the inputs for inferencing.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Storing the original threshold and nms overlap values to restore later and applying the user provided values\n",
    "        self.nms_thres = self.model.roi_heads.nms_thresh\n",
    "        self.thresh = self.model.roi_heads.score_thresh\n",
    "        self.model.roi_heads.nms_thresh = nms_overlap\n",
    "        self.model.roi_heads.score_thresh = thresh\n",
    "\n",
    "        # Denormalize from imagenet_stats\n",
    "        imagenet_stats = [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]\n",
    "        mean = self.torch.tensor(imagenet_stats[0], dtype=self.torch.float32).to(xb.device)\n",
    "        std  = self.torch.tensor(imagenet_stats[1], dtype=self.torch.float32).to(xb.device)\n",
    "        xb = (xb.permute(0, 2, 3, 1)*std + mean).permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Model inputs are required in the list format\n",
    "        return list(xb)\n",
    "    \n",
    "\n",
    "    def transform_input_multispectral(self, xb, thresh=0.5, nms_overlap=0.1):\n",
    "        \"\"\"\n",
    "        Function to transform the multispectral inputs for inferencing.\n",
    "        \"\"\"\n",
    "\n",
    "        # Storing the original threshold and nms overlap values to restore later and applying the user provided values\n",
    "        self.nms_thres = self.model.roi_heads.nms_thresh\n",
    "        self.thresh = self.model.roi_heads.score_thresh\n",
    "        self.model.roi_heads.nms_thresh = nms_overlap\n",
    "        self.model.roi_heads.score_thresh = thresh\n",
    "        \n",
    "        # Model inputs are required in the list format\n",
    "        return list(xb)\n",
    "    \n",
    "\n",
    "    def get_model(self, data, backbone=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Function that defines the model architecture using FasterRCNN model available in the torchvision library.\n",
    "        An option to select different backbones from the resnet family has also been added.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fasterrcnn_kwargs, kwargs = self.fastai.core.split_kwargs_by_func(kwargs,\n",
    "                                                                         self.torchvision.models.detection.FasterRCNN.__init__)\n",
    "        if backbone is None:\n",
    "            backbone = self.torchvision.models.resnet50\n",
    "\n",
    "        elif type(backbone) is str:\n",
    "            if hasattr(self.torchvision.models, backbone):\n",
    "                backbone = getattr(self.torchvision.models, backbone)\n",
    "            elif hasattr(self.torchvision.models.detection, backbone):\n",
    "                backbone = getattr(self.torchvision.models.detection, backbone)\n",
    "        else:\n",
    "            backbone = backbone\n",
    "        pretrained_backbone = kwargs.get('pretrained_backbone', True)\n",
    "        assert type(pretrained_backbone) == bool\n",
    "        if backbone.__name__ == 'resnet50':\n",
    "            model = self.torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=pretrained_backbone,\n",
    "                                                                              min_size = 1.5*data.chip_size,\n",
    "                                                                              max_size = 2*data.chip_size,\n",
    "                                                                              **self.fasterrcnn_kwargs)\n",
    "        elif backbone.__name__ in ['resnet18','resnet34']:\n",
    "            backbone_small = self.fastai.vision.learner.create_body(backbone, pretrained=pretrained_backbone)\n",
    "            backbone_small.out_channels = 512\n",
    "            model = self.torchvision.models.detection.FasterRCNN(backbone_small,\n",
    "                                                                 91,\n",
    "                                                                 min_size = 1.5*data.chip_size,\n",
    "                                                                 max_size = 2*data.chip_size,\n",
    "                                                                 **self.fasterrcnn_kwargs)\n",
    "        else:\n",
    "            backbone_fpn = self.torchvision.models.detection.backbone_utils.resnet_fpn_backbone(\n",
    "                backbone.__name__,\n",
    "                pretrained = pretrained_backbone\n",
    "            )\n",
    "            model = self.torchvision.models.detection.FasterRCNN(backbone_fpn,\n",
    "                                                                 91,\n",
    "                                                                 min_size = 1.5*data.chip_size,\n",
    "                                                                 max_size = 2*data.chip_size,\n",
    "                                                                 **self.fasterrcnn_kwargs)\n",
    "\n",
    "        in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "        model.roi_heads.box_predictor = self.torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, len(data.classes))\n",
    "        \n",
    "        if data._is_multispectral:\n",
    "            model.transform.image_mean = [0]*len(data._extract_bands)\n",
    "            model.transform.image_std = [1]*len(data._extract_bands)\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def loss(self, model_output, *model_target):\n",
    "        \"\"\"\n",
    "        Function to define the loss calculations.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Torchvision's FasterRCNN model itself returns the loss in training mode.\n",
    "        # Therefore, we don't need to re-define the loss calculation here.\n",
    "        final_loss = 0.\n",
    "        for i in model_output.values():\n",
    "            i[self.torch.isnan(i)] = 0.\n",
    "            i[self.torch.isinf(i)] = 0.\n",
    "            final_loss += i\n",
    "        \n",
    "        return final_loss\n",
    "   \n",
    "\n",
    "    def post_process(self, pred, nms_overlap, thres, chip_size, device):\n",
    "        \"\"\"\n",
    "        Function to post process the output of the model in validation/infrencing mode.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Restoring the original threshold and nms_thresh\n",
    "        self.model.roi_heads.score_thresh = self.thresh\n",
    "        self.model.roi_heads.nms_thresh = self.nms_thres\n",
    "        # Torchvision's FasterRCNN returns the otput after applying confidence threshold and nms threshold,\n",
    "        # therefore, `nms_overlap` and `thresh` are not used in this function.\n",
    "\n",
    "        post_processed_pred = []\n",
    "        \n",
    "        for p in pred:\n",
    "            bbox, label, score = p[\"boxes\"], p[\"labels\"], p[\"scores\"]\n",
    "            # Convert bboxes to range -1 to 1.\n",
    "            bbox = bbox/(chip_size/2) - 1\n",
    "            # Convert bboxes to format [y1,x1,y2,x2]\n",
    "            bbox = self.torch.index_select(bbox, 1, self.torch.tensor([1,0,3,2]).to(bbox.device))\n",
    "            # Create a tuple of bboxes, labels and scores for each image and append it in a list\n",
    "            post_processed_pred.append((bbox.data.to(device), label.to(device), score.to(device)))\n",
    "            \n",
    "        return post_processed_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created our model class, we need to save it as a Python script file. The model class will then need to be imported from the file using the Python statement `from fasterrcnn import MyFasterRCNN`. Note, here the file `fasterrcnn.py` is present in the current working directory, which is suggested. We need to provide the correct path depending on where we saved the file.\n",
    "\n",
    "Next, we'll need to import `prepare_data` function and the `ModelExtension` class from `arcgis.learn`. The `prepare_data` function will be used to create a fastai databunch and `ModelExtension` class will be used to initialize our custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasterrcnn import MyFasterRCNN\n",
    "\n",
    "from arcgis.learn import prepare_data, ModelExtension\n",
    "\n",
    "data = prepare_data(r'path\\to\\data')\n",
    "\n",
    "model = ModelExtension(data, MyFasterRCNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can continue with the usual workflow of using an `arcgis.learn` deep learning model. Refer to the [Detecting Swimming Pools using Deep Learning sample](https://developers.arcgis.com/python/sample-notebooks/detecting-swimming-pools-using-satellite-image-and-deep-learning/) sample notebook to see the workflow for an **object detection** model and the [Land Conver Classification using Satellite Imagery and Deep Learning](https://developers.arcgis.com/python/sample-notebooks/land-cover-classification-using-unet/) sample notebook for the workflow for a **pixel classification** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
