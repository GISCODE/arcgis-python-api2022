{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# How RetinaNet works?"]}, {"cell_type": "markdown", "metadata": {"toc": true}, "source": ["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n", "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#How-RetinaNet-works?\" data-toc-modified-id=\"How-RetinaNet-works?-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>How RetinaNet works?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#What's-a-Feature-Pyramid-Network?\" data-toc-modified-id=\"What's-a-Feature-Pyramid-Network?-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>What's a Feature Pyramid Network?</a></span></li><li><span><a href=\"#RetinaNet-architecture\" data-toc-modified-id=\"RetinaNet-architecture-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>RetinaNet architecture</a></span></li><li><span><a href=\"#Focal-Loss\" data-toc-modified-id=\"Focal-Loss-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Focal Loss</a></span></li><li><span><a href=\"#Implementation-in-arcgis.learn\" data-toc-modified-id=\"Implementation-in-arcgis.learn-1.1.4\"><span class=\"toc-item-num\">1.1.4&nbsp;&nbsp;</span>Implementation in <code>arcgis.learn</code></a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-1.1.5\"><span class=\"toc-item-num\">1.1.5&nbsp;&nbsp;</span>References</a></span></li></ul></li></ul></li></ul></div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Introduction"]}, {"cell_type": "markdown", "metadata": {}, "source": ["RetinaNet is one of the best one-stage object detection models that has proven to work well with dense and small scale objects. For this reason, it has become a popular object detection model to be used with aerial and satellite imagery."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><img src=\"../../static/img/retinanet_pools_detection.png\" height=\"300\" width=\"300\" /></center>\n", "<center>Figure 1. Swimming Pools detection using RetinaNet</center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["RetinaNet has been formed by making two improvements over existing single stage object detection models - Feature Pyramid Networks (FPN) [1] and Focal Loss [2]. Before diving into RetinaNet\u2019s architecture, let's first understand FPN."]}, {"cell_type": "markdown", "metadata": {}, "source": ["> To follow the guide below, we assume that you have some basic understanding of the convolutional neural networks (CNN) concept. You can refresh your CNN knowledge by going through this short paper \u201c[A guide to convolution arithmetic for deep learning](https://arxiv.org/pdf/1603.07285.pdf)\u201d.\n", "It is also assumed that you have an understanding of object detection models. Please refer to the guide \"[How SSD Works](https://developers.arcgis.com/python/guide/how-ssd-works/)\" where concepts such as anchor boxes and feature maps have been well explained."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### What's a Feature Pyramid Network?\n", "\n", "Traditionally, in computer vision, featurized image pyramids have been used to detect objects with varying scales in an image. Featurized image pyramids are feature pyramids built upon image pyramids. This means one would take an image and subsample it into lower resolution and smaller size images (thus, forming a pyramid). Hand-engineered features are then extracted from each layer in the pyramid to detect the objects [1]. This makes the pyramid scale-invariant. But, this process is compute and memory intensive.\u00a0\n", "\n", "With the advent of deep learning, these hand-engineered features were replaced by CNNs. Later, the pyramid itself was derived from the inherent pyramidal hierarchical structure of the CNNs. In a CNN architecture, the output size of feature maps decreases after each successive block of convolutional operations, and forms a pyramidal structure."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><img src=\"../../static/img/feature_pyramid_network.png\" /></center>\n", "<center>Figure 2. Different types of pyramid architectures [1]</center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There have been various architectures that utilize the pyramid structure (Figure 2). The (a) Featurized image pyramid, as we have discussed, is compute intensive. (b) Single (scale) feature maps have been used for faster detections. Even though they are robust and fast, pyramids are still needed to get the most accurate results [1]. (c) Pyramidal feature hierarchy has been utilized by models such as Single Shot detector, but it doesn't reuse the multi-scale feature maps from different layers. (d) Feature Pyramid Network (FPN) makes up for the shortcomings in these variations. FPN creates an architecture with rich semantics at all levels as it combines low-resolution semantically strong features with high-resolution semantically weak features [1]. This is achieved by creating a top-down pathway with lateral connections to bottom-up convolutional layers.\n", "\n", "Top-down pathway, bottom-up pathway and lateral connections will be better understood in the next section when we take a look at the RetinaNet architecture. RetinaNet incorporates FPN and adds classification and regression subnetworks to create an object detection model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### RetinaNet architecture"]}, {"cell_type": "markdown", "metadata": {}, "source": ["There are four major components of a RetinaNet model architecture (Figure 3):\n", "\n", "   a) Bottom-up Pathway - The backbone network (e.g. ResNet) which calculates the feature maps at different scales, irrespective of the input image size or the backbone.  \n", "   b) Top-down pathway and Lateral connections - The top down pathway upsamples the spatially coarser feature maps from higher pyramid levels, and the lateral connections merge the top-down layers and the bottom-up layers with the same spatial size.  \n", "   c) Classification subnetwork - It predicts the probability of an object being present at each spatial location for each anchor box and object class.  \n", "   d) Regression subnetwork - It's regresses the offset for the bounding boxes from the anchor boxes for each ground-truth object."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<center><img src=\"../../static/img/retinanet.png\" /></center>\n", "<center>Figure 3. RetinaNet model architecture [2]</center>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Focal Loss\n", "\n", "Focal Loss (FL) is an enhancement over Cross-Entropy Loss (CE) and is introduced to handle the class imbalance problem with single-stage object detection models. Single Stage models suffer from a extreme foreground-background class imbalance problem due to dense sampling of anchor boxes (possible object locations) [2]. In RetinaNet, at each pyramid layer there can be thousands of anchor boxes. Only a few will be assigned to a ground-truth object while the vast majority will be background class. These easy examples (detections with high probabilities) although resulting in small loss values can collectively overwhelm the model. Focal Loss reduces the loss contribution from easy examples and increases the importance of correcting missclassified examples."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Implementation in `arcgis.learn`\n", "\n", "You can create a RetinaNet model in `arcgis.learn` using a single line of code.  \n", "```\n", "model = RetinaNet(data)\n", "```\n", "\n", "The important parameters to be passed are:\n", "* The ``data`` that you would have prepared in the earlier steps.\n", "* A ``backbone`` model from the ResNet family. The default is set to ResNet50.\n", "* ``scales`` of anchor boxes. The default is set to [2^0, 2^\u2153, 2^\u2154 ] which works well with most of the objects in any datasets. You can change the scales according to the size of objects in your dataset. \n", "* aspect ``ratios`` of anchor boxes. The default is set to  [0.5, 1, 2] which means the anchor boxes will be of aspect ratios 1:2, 1:1, 2:1. You can modify the ratios according to the shape of the objects of interest.\n", "\n", "Tuning these parameters is based on intuition built through understanding the model and experimenting on the dataset.\n", "\n", "For more information about the API, please go to the [API reference](https://developers.arcgis.com/python/api-reference/arcgis.learn.toc.html#retinanet)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### References"]}, {"cell_type": "markdown", "metadata": {}, "source": ["* [1] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan: \u201cFeature Pyramid Networks for Object Detection\u201d, 2016; [http://arxiv.org/abs/1612.03144 arXiv:1612.03144].\n", "* [2] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He: \u201cFocal Loss for Dense Object Detection\u201d, 2017; [http://arxiv.org/abs/1708.02002 arXiv:1708.02002]."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.2"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": true, "toc_position": {}, "toc_section_display": true, "toc_window_display": true}}, "nbformat": 4, "nbformat_minor": 2}