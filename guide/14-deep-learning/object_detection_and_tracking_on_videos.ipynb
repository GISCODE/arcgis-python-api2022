{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Object detection and tracking on videos\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The [Object detection with arcgis.learn](https://developers.arcgis.com/python/guide/object-detection/) section of this guide explains how object detection models can be trained and used to  extract the location of detected objects from imagery. This section of the guide explains how they can be applied to videos, for both detecting objects in a video, as well as for tracking them."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Table of Contents\n", "* [Object detection in videos](#Object-detection-in-videos)\n", "* [Object tracking](#Object-tracking)\n", "* [Object detection and tracking using predict_video function](#Object-detection-and-tracking-using-predict_video-function)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Object detection in videos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Object detection models can be used to detect objects in videos using the [``predict_video``](https://developers.arcgis.com/python/api-reference/arcgis.learn.html?highlight=predict_video#arcgis.learn.SingleShotDetector.predict_video) function. This function applies the model to each frame of the video, and provides the classes and bounding boxes of detected objects in each frame. The information is stored in a metadata file. The detected objects can also be visualized on the video, by specifying the `visualize=True` flag. By default, the output video is saved in the original video's directory.\n", " \n", "The metadata file is a comma-separated values (CSV) file, containing metadata about the video frames for specific times. This function updates the CSV file by encoding object detections in the MISB 0903 standard in the `vmtilocaldataset` column. When multiplexed with the original video, this enables the object detections to be visualized in ArcGIS Pro, using its support for Full Motion Video (FMV) and VMTI (video moving target indications) metadata. To learn more about it, read [here](https://pro.arcgis.com/en/pro-app/tool-reference/image-analyst/video-multiplexer.htm).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Object tracking"]}, {"cell_type": "markdown", "metadata": {}, "source": ["When detecting objects in a video, we are often interested in knowing how many objects are there and what tracks they follow. As an example, in a video from a traffic camera installed at intersection, we may be interested in counting the number and types of vehicles crossing the intersection. Optionally, in a video captured from a drone, we might be interested in counting or tracking individual objects as they move around.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Object tracking is a process of:\n", "\n", "* Taking an initial set of object detections (such as an input set of bounding box coordinates)\n", "* Creating a unique ID for each of the initial detections\n", "* And then tracking each of the objects as they move around frames in a video, maintaining the assignment of unique IDs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Object tracking in `arcgis.learn` is based on SORT(Simple Online Realtime Tracking) algorithm. This algorithm combines **Kalman-filtering and Hungarian Assignment Algorithm**\n", "\n", "**Kalman Filter** is used to estimate the position of a tracker while **Hungarian Algorithm** is used to assign trackers to a new detection."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Kalman Filter\n", "\n", "Kalman filtering uses a series of measurements observed over time and produces estimates of unknown variables by estimating a joint probability distribution over the variables for each timeframe. The filter is named after Rudolf E. K\u00e1lm\u00e1n, one of the primary developers of its theory.\n", "\n", "Our state contains 8 variables; `(u,v,a,h,u\u2019,v\u2019,a\u2019,h\u2019)` where `(u,v)` are centres of the bounding boxes, a is the aspect ratio and h, the height of the image. The other variables are the respective velocities of the variables.\n", "\n", "A Kalman Filter is used on every bounding box, so it comes after a box has been matched with a tracker. When the association is made, predict and update functions are called. \n", "\n", "* Predict: Prediction step is matrix multiplication that will tell us the position of our bounding box at time t based on its position at time t-1.\n", "\n", "* Update: Update phase is a correction step. It includes the new measurement from the Object Detection model and helps improve our filter.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Hungarian Assignment Algorithm\n", "\n", "The Hungarian algorithm, also known as Kuhn-Munkres algorithm, can associate an obstacle from one frame to another, based on a score such as Intersection over Union (IoU). \n", "\n", "We iterate through the list of trackers and detections and assign a tracker to each detection on the basis of IoU scores.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The general process is to detect obstacles using an object detection algorithm, match these bounding box with former bounding boxes we have using The Hungarian Algorithm and then predict future bounding box positions or actual positions using Kalman Filters. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Object detection and tracking using `predict_video` function"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "The following options/parameters can be specified in the predict video function by the user:\n", " \n", "- The final saved VMTI can be multiplexed with the input video by passing the ``multiplex=True`` flag. The multiplexed video can be saved at the path specified in ``multiplex_file_path``. By default, the video gets saved in the original video's directory.\n", "\n", "The `track=True` parameter can be used to track detected objects in the video. When tracking the detected objects, the following ``tracker_options`` can be specified as a ``dict``: \n", "\n", "  * ``assignment_iou_thrd`` - There might be multiple trackers detecting and tracking objects. The Intersection over Union (iou) threshold can be set to assign a tracker with the mentioned threshold value. \n", "  * ``vanish_frames`` - Then the number of frames the object remains absent from the frame can be mentioned for it to be considered as vanished. \n", "  * ``detect_frames`` - Also the number of frames an object remains present in the frame to start tracking it.\n", "    \n", "Additionally, the detections can be visualized on an output video that this function can create, if passed the `visualize=True` parameter. When visualizing the detected objects, the following ``visual_options`` can be specified to display scores, labels, the color of the predictions, thickness and font face to show the labels:\n", "\n", "   * `show_scores` - To view scores on predictions\n", "   * `show_labels` - To view labels on predictions\n", "   * `thickness` - To set the thickness level of box\n", "   * `fontface` - Fontface value from opencv values\n", "   * `color` - (B, G, R) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["The example below shows how a trained model can be used to detect objects in a video:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mdl = model.from_model(r'\\path\\to\\model\\model.emd')"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["mdl.predict_video(\n", "    input_video_path=r'\\path\\to\\video.mp4', \n", "    metadata_file=r'\\path\\to\\metadata\\file.csv',\n", "    visualize=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The following example shows how the detected objects can be additionally tracked as well as multiplexed. Additionally, it creates an output video that visualizes the detected objects using the specified `visual_options`:"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["mdl.predict_video(\n", "    input_video_path=r'\\path\\to\\input_video.mp4', \n", "    metadata_file=r'\\path\\to\\metadata\\metedata_file.csv',\n", "    track=True,\n", "    output_file_path=r'\\path\\to\\output\\output_file.mp4',\n", "    multiplex=True,\n", "    multiplex_file_path=r'\\path\\to\\output\\multiplexed_file.mp4',\n", "    tracker_options={'assignment_iou_thrd': 0.3, 'vanish_frames': 40, 'detect_frames': 10},\n", "    visual_options={'show_scores': True, 'show_labels': True, 'thickness': 2, 'fontface': 0, 'color': (255, 255, 255)})"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<video width=\"100%\" height=\"450\" loop=\"loop\" controls src=\"data/test_predictions.mp4\" />"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can refer to [this sample notebook](https://github.com/Esri/arcgis-python-api/blob/master/samples/04_gis_analysts_data_scientists/automate_road_surface_investigation_using_deep_learning.ipynb) for a detailed workflow that automates road surface investigation using a video.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## References\n", "\n", "[1] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He: \u201cFocal Loss for Dense Object Detection\u201d, 2017; [http://arxiv.org/abs/1708.02002 arXiv:1708.02002].\n", "\n", "[2] https://towardsdatascience.com/computer-vision-for-tracking-8220759eee85\n", "\n", "[3] https://arxiv.org/abs/1602.00763"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}